{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe542f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f91b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desktop-1qpu8cc\\¤Ё ­  вагЎЁ­ \n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1d4cd",
   "metadata": {},
   "source": [
    "# Заголовок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e168244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17f04b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimple_app\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/clients.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('simple_app').master('local[*]').getOrCreate()\n",
    "df = spark.read.format('csv').option('inferSchema', 'true').option('header', 'true').load('../data/clients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf42ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b705ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab05d87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>929135.0</td>\n",
       "      <td>464567.000000</td>\n",
       "      <td>268218.315518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232283.5</td>\n",
       "      <td>464567.0</td>\n",
       "      <td>696850.5</td>\n",
       "      <td>929134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_id</th>\n",
       "      <td>929135.0</td>\n",
       "      <td>500.573223</td>\n",
       "      <td>288.664178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_category</th>\n",
       "      <td>929135.0</td>\n",
       "      <td>9.348996</td>\n",
       "      <td>7.495062</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>929135.0</td>\n",
       "      <td>1143.750824</td>\n",
       "      <td>3948.343863</td>\n",
       "      <td>50.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>149891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count           mean            std   min       25%  \\\n",
       "id                929135.0  464567.000000  268218.315518   0.0  232283.5   \n",
       "client_id         929135.0     500.573223     288.664178   1.0     251.0   \n",
       "product_category  929135.0       9.348996       7.495062   1.0       5.0   \n",
       "amount            929135.0    1143.750824    3948.343863  50.0      83.0   \n",
       "\n",
       "                       50%       75%       max  \n",
       "id                464567.0  696850.5  929134.0  \n",
       "client_id            501.0     750.0    1000.0  \n",
       "product_category       6.0      11.0      29.0  \n",
       "amount               347.0    1042.0  149891.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958ea1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>client_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_company</th>\n",
       "      <th>subtype</th>\n",
       "      <th>amount</th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34478</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>1209</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  client_id  product_category product_company     subtype  amount  \\\n",
       "0  34478         37                29             NaN  Автоплатеж    1209   \n",
       "\n",
       "                  date transaction_type  \n",
       "0  2020-01-01 00:00:00         Negative  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5f65ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\3059667985.py:1: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, 'date'] = pd.to_datetime(df.loc[:, 'date'])\n"
     ]
    }
   ],
   "source": [
    "df.loc[:, 'date'] = pd.to_datetime(df.loc[:, 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4b5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fake_df(df, shift, max_id):\n",
    "\n",
    "    df_fake = df.copy()\n",
    "    df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
    "    df_fake.loc[:, 'id'] += df['id'].max() + 1\n",
    "    df_fake.loc[:, 'amount'] *= np.random.choice(range(11, 20, 1))/10.\n",
    "\n",
    "    df_fake.loc[:, 'client_id'] = np.where(df_fake['client_id'] < 100, df_fake['client_id'] + np.random.choice(range(1, 10, 1)), df['client_id'])\n",
    "    df_fake.loc[:, 'client_id'] = np.where(df_fake['client_id'] > 900, df_fake['client_id'] - np.random.choice(range(1, 10, 1)), df['client_id'])\n",
    "    df_fake.loc[:, 'client_id'] = np.where((df_fake['client_id'] >= 100) & (df_fake['client_id'] < 500) , df_fake['client_id'] + np.random.choice(range(1, 10, 1)), df['client_id'])\n",
    "    df_fake.loc[:, 'client_id'] = np.where((df_fake['client_id'] >= 500) & (df_fake['client_id'] <= 900) , df_fake['client_id'] - np.random.choice(range(1, 10, 1)), df['client_id'])\n",
    "\n",
    "    return df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c081d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>client_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_company</th>\n",
       "      <th>subtype</th>\n",
       "      <th>amount</th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34478</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>1209</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65406</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Подписка</td>\n",
       "      <td>169</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82186</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>3711</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83113</td>\n",
       "      <td>89</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>6443</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131373</td>\n",
       "      <td>141</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>7319</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  client_id  product_category product_company     subtype  amount  \\\n",
       "0   34478         37                29             NaN  Автоплатеж    1209   \n",
       "1   65406         71                 4   YouTube Music    Подписка     169   \n",
       "2   82186         88                29             NaN  Автоплатеж    3711   \n",
       "3   83113         89                29             NaN  Автоплатеж    6443   \n",
       "4  131373        141                29             NaN  Автоплатеж    7319   \n",
       "\n",
       "        date transaction_type  \n",
       "0 2020-01-01         Negative  \n",
       "1 2020-01-01         Negative  \n",
       "2 2020-01-01         Negative  \n",
       "3 2020-01-01         Negative  \n",
       "4 2020-01-01         Negative  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38ecd270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>client_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_company</th>\n",
       "      <th>subtype</th>\n",
       "      <th>amount</th>\n",
       "      <th>date</th>\n",
       "      <th>transaction_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34478</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65406</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Подписка</td>\n",
       "      <td>169.0</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82186</td>\n",
       "      <td>88</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>3711.0</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83113</td>\n",
       "      <td>89</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>6443.0</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131373</td>\n",
       "      <td>141</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Автоплатеж</td>\n",
       "      <td>7319.0</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078750</th>\n",
       "      <td>940772</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>Стрелка</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2032-12-31 23:58:04</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078751</th>\n",
       "      <td>1108473</td>\n",
       "      <td>194</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>59.4</td>\n",
       "      <td>2032-12-31 23:58:11</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078752</th>\n",
       "      <td>1440379</td>\n",
       "      <td>546</td>\n",
       "      <td>3</td>\n",
       "      <td>Ситимобил</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>344.3</td>\n",
       "      <td>2032-12-31 23:58:13</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078753</th>\n",
       "      <td>1143928</td>\n",
       "      <td>232</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>95.7</td>\n",
       "      <td>2032-12-31 23:59:03</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078754</th>\n",
       "      <td>1363908</td>\n",
       "      <td>469</td>\n",
       "      <td>2</td>\n",
       "      <td>Яндекс.Лавка</td>\n",
       "      <td>Покупка</td>\n",
       "      <td>4384.6</td>\n",
       "      <td>2032-12-31 23:59:12</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12078755 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  client_id  product_category product_company     subtype  \\\n",
       "0           34478         37                29             NaN  Автоплатеж   \n",
       "1           65406         71                 4   YouTube Music    Подписка   \n",
       "2           82186         88                29             NaN  Автоплатеж   \n",
       "3           83113         89                29             NaN  Автоплатеж   \n",
       "4          131373        141                29             NaN  Автоплатеж   \n",
       "...           ...        ...               ...             ...         ...   \n",
       "12078750   940772         13                 6         Стрелка     Покупка   \n",
       "12078751  1108473        194                 6             NaN     Покупка   \n",
       "12078752  1440379        546                 3       Ситимобил     Покупка   \n",
       "12078753  1143928        232                 6             NaN     Покупка   \n",
       "12078754  1363908        469                 2    Яндекс.Лавка     Покупка   \n",
       "\n",
       "          amount                date transaction_type  \n",
       "0         1209.0 2020-01-01 00:00:00         Negative  \n",
       "1          169.0 2020-01-01 00:00:00         Negative  \n",
       "2         3711.0 2020-01-01 00:00:00         Negative  \n",
       "3         6443.0 2020-01-01 00:00:00         Negative  \n",
       "4         7319.0 2020-01-01 00:00:00         Negative  \n",
       "...          ...                 ...              ...  \n",
       "12078750    55.0 2032-12-31 23:58:04         Negative  \n",
       "12078751    59.4 2032-12-31 23:58:11         Negative  \n",
       "12078752   344.3 2032-12-31 23:58:13         Negative  \n",
       "12078753    95.7 2032-12-31 23:59:03         Negative  \n",
       "12078754  4384.6 2032-12-31 23:59:12         Negative  \n",
       "\n",
       "[12078755 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = df.copy()\n",
    "\n",
    "for i in range(1, 13):\n",
    "\n",
    "    max_id = df['id'].max()\n",
    "    df_fake = make_fake_df(df_raw, i, max_id)\n",
    "    df = pd.concat(objs=[df, df_fake])\n",
    "\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62bb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\1613476043.py:1: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, 'date'] -= pd.offsets.DateOffset(years=10)\n"
     ]
    }
   ],
   "source": [
    "df.loc[:, 'date'] -= pd.offsets.DateOffset(years=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54074757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n",
      "C:\\Users\\Диана Трубина\\AppData\\Local\\Temp\\ipykernel_29768\\535486400.py:4: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df_fake.loc[:, 'date'] += pd.offsets.DateOffset(years=shift)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.17 GiB for an array with shape (1, 157023815) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     max_id \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m      6\u001b[0m     df_fake \u001b[38;5;241m=\u001b[39m make_fake_df(df_raw, i, max_id)\n\u001b[1;32m----> 7\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_fake\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:381\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    369\u001b[0m     objs,\n\u001b[0;32m    370\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    620\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py:233\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    231\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py:577\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    574\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_axis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat_values\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:151\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m    148\u001b[0m             to_concat \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m to_concat]\n\u001b[0;32m    149\u001b[0m             kinds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kinds \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# GH#39817\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehavior when concatenating bool-dtype and numeric-dtype arrays is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated; in a future version these will cast to object dtype \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.17 GiB for an array with shape (1, 157023815) and data type object"
     ]
    }
   ],
   "source": [
    "df_raw = df.copy()\n",
    "\n",
    "for i in range(1, 13):\n",
    "\n",
    "    max_id = df['id'].max()\n",
    "    df_fake = make_fake_df(df_raw, i, max_id)\n",
    "    df = pd.concat(objs=[df, df_fake])\n",
    "\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304ca13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
